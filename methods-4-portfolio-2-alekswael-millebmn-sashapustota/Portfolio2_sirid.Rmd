---
title: "Methods 4 -- Portfolio Assignment 2"
output: html_notebook
---

```{r}
pacman::p_load(tidyverse, rethinking, dagitty,GGally)
```


- *Type:* Group assignment
- *Due:* 3 April 2022, 23:59

Hello CogSci\'s :)

In this portfolio, you are asked to do four tasks:

\- Make a DAG for something

\- Simulate data that fits the DAG

\- Use linear models to confirm that the DAG fits the data

\- Mess it up.

Each of the four tasks have some sub-steps.\
Report briefly what you find, for example in a markdown document, for
example called report.md so that the poor TA can easily get an overview
before looking in your code :)

Then you can also make a (brief!) explanation of the phenomenon you are
DAGGIN, simulating and modelling.

Looking forward !

## Task 1: The DAG

\- **Come up with an** incredibly interesting and scientifically
important made-up **example** for a phenomenon to investigate. Decide on
two variables (an outcome and a predictor) that you would like to
investigate the relation between. If in doubt, you **can be inspired by
Peter\'s amazing example** on the next page.

\- **Make a DAG** for the phenomenon. Make it medium complicated: that
means, make sure there are some different kinds of relations (see next
step). Change it if you don\'t get anything interesting for the next
steps.\
**Draw it** somehow (on paper, in R, laser engraved in diamond).\
**Code it** in dagitty (this is a nice tool:
http://dagitty.net/dags.html )

\- Find **elemental forms of variable relations** in the DAG
(i.e., forks, pipes, colliders, and their descendants).

\- Find out **what variables to include (and not include)** in a
multiple linear regression to avoid \'back door\' (AKA non-causal)
paths. Do this first with your eyes and your mind. Then you can use
dagitty\'s function `adjustmentSets()`.

\- Find out which **conditional independencies** the DAG implies. First
with the mind, then with daggity\'s function
`impliedConditionalIndependencies()`.

\- Find the full list of **Markov equivalent** DAGS. Use daggity\'s
function `equivalentGraphs()`.

### Making the DAG
```{r Making a DAG}
DAG <- dagitty( "dag {
D <- S -> P
P -> M -> D -> B
P -> B <- M 
}")
drawdag(DAG)

adjustmentSets(DAG, "M", "B") 

impliedConditionalIndependencies(DAG)

equivalentDAGs(DAG)
```

## Task 2: The data

\- **Simulate some data that fits the DAG.** There are many ways to do
this. A simple way is just to sample one variable from a normal
distribution which has another variable as mean. McElreath does this in
the book a few times, and you can use this as inspiration.

### Simulate data
```{r Simulating data}
n <- 300
set.seed(3)

d_sim <- 
  tibble(S = rnorm(n, mean=0, sd=1)) %>% 
  mutate(P = rnorm(n, mean=S, sd=1)) %>% 
  mutate(M = rnorm(n, mean=P, sd=1)) %>%
  mutate(D = rnorm(n, mean=S+M, sd=1)) %>% 
  mutate(B = rnorm(n, mean=D+M+P, sd=1))

GGally::ggpairs(d_sim)

precis(d_sim)
```

### Model definition
$$B ∼ Poisson(μ, σ) \\ μ = a + βmMi + βdDi \\  a ∼ Normal(0,0.02) \\ βm ∼ Normal(0,0.5) \\ βd ∼ Normal(0,0.5) \\ σ ∼ Exponential(1) $$
*description of priors* (p. 146)
What about those priors? Since the outcome and the predictor are both standardized, the
intercept α should end up very close to zero. What does the prior slope βA imply? If βA = 1,
that would imply that a change of one standard deviation in age at marriage is associated
likewise with a change of one standard deviation in divorce. To know whether or not that is
a strong relationship, you need to know how big a standard deviation of age at marriage is:
sd( d$MedianAgeMarriage )
So when βA = 1, a change of 1.2 years in median age at marriage is associated with a full
standard deviation change in the outcome variable. That seems like an insanely strong relationship.
The prior above thinks that only 5% of plausible slopes more extreme than 1. We’ll
simulate from these priors in a moment, so you can see how they look in the outcome space.


## Task 3: Statistics

\- Run **multiple linear regression**s to **test the conditional
independencies** **implied by your DAG**. Make sure to avoid backdoor
paths. See that the linear model shows the conditional independencies
implied by your DAG, implying that the data and the DAG are compatible
(if the linear model doesn\'t show the conditional independencies
implied by the DAG, the data and the DAG doesn\'t fit).

```{r testing B _||_ S | D, M, P}
# testing B _||_ S | D, M, P

# B ~ A 
m1 <- quap(
  alist(
    B ~ dnorm(mu, sigma),
    mu <- a + bS*S,
    a ~ dnorm(0, 0.2),
    bS ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d_sim
)

# B ~ S + D + M + P
m2 <- quap(
  alist(
    B ~ dnorm(mu, sigma),
    mu <- a + bS*S + bD*D + bM*M + bP*P,
    a ~ dnorm(0, 0.2),
    bS ~ dnorm(0, 0.5),
    bD ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    bP ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d_sim
)

precis(m1)
precis(m2)
coeftab_plot(coeftab(m1, m2) , pars=c("bS"))
```

```{r testing D _||_ P | M, S}
# testing D _||_ P | M, S

# D ~ P 
m3 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bP*P,
    a ~ dnorm(0, 0.2),
    bP ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d_sim
)

# D ~ P + M + S
m4 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bP*P + bM*M + bS*S,
    a ~ dnorm(0, 0.2),
    bP ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    bS ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d_sim
)

precis(m3)
precis(m4)
coeftab_plot(coeftab(m3, m4) , pars=c("bP"))
```

```{r testing M _||_ S | D}
# testing M _||_ S | D

# M ~ S
m5 <- quap(
  alist(
    M ~ dnorm(mu, sigma),
    mu <- a + bS*S,
    a ~ dnorm(0, 0.2),
    bS ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d_sim
)

# M ~ S + D
m6 <- quap(
  alist(
    M ~ dnorm(mu, sigma),
    mu <- a + bS*S + bD*D,
    a ~ dnorm(0, 0.2),
    bS ~ dnorm(0, 0.5),
    bD ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d_sim
)

precis(m5)
precis(m6)
coeftab_plot(coeftab(m5, m6) , pars=c("bS"))
```


## Task 4: Messing it up

\- Try and **deliberately have an open back door path** and see if you
can get wrong inference.

```{r}
# B ~ M + D
mB_MD <- quap(
  alist(
    B ~ dnorm(mu, sigma),
    mu <- a + bM*M + bD*D,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    bD ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d_sim
)

# B ~ M + D + P
mB_MDP <- quap(
  alist(
    B ~ dnorm(mu, sigma),
    mu <- a + bM*M + bD*D + bP*P,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    bD ~ dnorm(0, 0.5),
    bP ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d_sim
)


precis(mB_MD)
precis(mB_MDP)
coeftab_plot(coeftab(mB_MD, mB_MDP), pars = c("bM", "bD"))
```

Our DAG tells us that we must stratify by P when modelling the effect of M on B to avoid a backdoor path. We tested this with two models; one including P and one without P. We see that the slopes for both M and D decrease when including P, so not including P will lead us to false inference about M and D's effect on B.


\- Try and deliberately **simulate some data that doesn\'t fit the
DAG**, or **create a new DAG that doesn\'t fit the data**.


We've chosen to create a new DAG and test this on our data. The new, fake DAG is identical to the real one except that we've removed the arrow from P to M. The real and fake DAGs are visualised below: 

```{r Comparing real and fake dag}
# real DAG (left)
DAG <- dagitty("dag {
D <- S -> P
P -> M -> D -> B
P -> B <- M 
}")
# fake DAG (right)
DAG_fake <- dagitty("dag {
D <- S -> P
M -> D -> B
P -> B <- M 
}")
DAGs <- list(DAG, DAG_fake)
drawdag(DAGs)
```


These are the implied conditional independencies we get from our two DAGs:

```{r implied conditional independencies}
print("Conditional Independencies for the REAL DAG:")
print(impliedConditionalIndependencies(DAG))

print("Conditional Independencies for the FAKE DAG:")
impliedConditionalIndependencies(DAG_fake)
```


\- Use the same approach as above to **show that the DAG is wrong** (by
showing that conditional independencies don\'t exist in the data, for
example).

We get the following three conditional independencies from our FAKE DAG that are NOT a part of the REAL DAG that we used for simulating our data. 
D _||_ P | S
M _||_ P
M _||_ S

When we test these three, we would suspect them not to show results. 

```{r testing conditional independencies for DAG_fake}
# testing D _||_ P | S

# D ~ P
m7 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bP*P,
    a ~ dnorm(0, 0.2),
    bP ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d_sim
)

# D ~ P + S
m8 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bP*P + bS*S,
    a ~ dnorm(0, 0.2),
    bP ~ dnorm(0, 0.5),
    bS ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d_sim
)

precis(m7)
precis(m8)
coeftab_plot(coeftab(m7, m8) , pars=c("bP"))

# testing M _||_ P
m9 <- quap(
  alist(
    M ~ dnorm(mu, sigma),
    mu <- a + bP*P,
    a ~ dnorm(0, 0.2),
    bP ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d_sim
)
precis(m9)

# testing M _||_ S
m10 <- quap(
  alist(
    M ~ dnorm(mu, sigma),
    mu <- a + bS*S,
    a ~ dnorm(0, 0.2),
    bS ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d_sim
)
precis(m10)
```

## Peter\'s perfectly optimal and extremely interesting example

*In a galaxy far, far away\...*

*It is a period of civil wars in the galaxy. A brave alliance of
underground freedom fighters has challenged the tyranny and oppression
of the awesome GALACTIC EMPIRE.*

*To crush the rebellion once and for all, the EMPIRE is constructing a
sinister new battle station. Powerful enough to destroy an entire
planet, its completion spells certain doom for the champions of
freedom.*

*The evil Emperor has figured out, however, that neither the battle
station nor the Force can help him avoid that more solar systems join
the rebellion. He has therefore hired a CogSci student to use causal
modelling and multiple linear regressions to investigate how the
activity of the Death Star and other factors affects the probability
that a given solar system will join the rebellion (this allows him to
more optimally suppress freedom in the Galaxy).*

*You are that student.*

We assume that the probability of a solar system joining the rebellion depends on\
- how many rebellion sympathizers there is in the system (more rebels
-\> higher probability of joining the rebellion)\
- how scared people are in the system (more scared -\> lower probability
of joining the rebellion).

How many rebellion sympathizers there is in a system depends on

\- crime levels (less crime -\> less rebellion sympathizers)

\- number of planets recently destroyed by the Death Star (more planets
destroyed -\> more rebellion sympathizers)

\- number of Jedis in the system (more Jedis -\> more rebellion
sympathizers)

How scared people are depends on

\- whether or not the Death Star is nearby (nearby -\> more scared)

\- how many jedis are in the system (more Jedis -\> less scared)

\- how much time Darth Vader has spent in the system recently (more time
-\> more scared)

Crime levels depend on

\- number of planets recently destroyed by the Death Star (more planets
destroyed -\> less crime)

And so on\....

